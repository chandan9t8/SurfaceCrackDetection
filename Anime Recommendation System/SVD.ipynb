{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandan9t8/UnivProjects/blob/main/Anime%20Recommendation%20System/SVD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxowP1MHhYs0",
        "outputId": "1e117f88-e577-4ad3-cb5e-683a46fa31d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import sklearn\n",
        "import random\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from surprise import Dataset\n",
        "from surprise import SVD\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "WdnsLF0khc2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import our cleaned ratings csv\n",
        "rating = pd.read_csv('/content/drive/MyDrive/Data Mining Project/cleaned_ratings.csv', index_col=0)\n",
        "real_rating = rating[rating['rating']!=-1]\n",
        "\n",
        "#Import our anime database csv\n",
        "anime = pd.read_csv('/content/drive/MyDrive/Data Mining Project/cleaned_anime.csv')\n",
        "\n",
        "#Create a dict of anime by anime id\n",
        "anime_by_id = anime[['anime_id', 'name']]\n",
        "anime_by_id = anime_by_id.set_index('anime_id').T.to_dict('list')"
      ],
      "metadata": {
        "id": "CPpx09QShojB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a surprise dataset for testing\n",
        "dataset = Dataset.load_from_df(real_rating,reader=Reader(rating_scale=(1,10)))\n",
        "\n",
        "# split data to trainset and testset\n",
        "train, test = train_test_split(dataset,test_size=0.3,random_state=42)"
      ],
      "metadata": {
        "id": "ctpI68iohw99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_factors = [50,100,150]\n",
        "n_epochs = [20,30]\n",
        "lr =[0.005,0.01]\n",
        "reg = [0.02,0.1]\n",
        "\n",
        "min_error = 1000\n",
        "\n",
        "for a in n_factors:\n",
        "    for b in n_epochs:\n",
        "        for c in lr:\n",
        "            for d in reg:\n",
        "                print(\"Training with %s factors, %s epochs, %s learning rate, %s regularizer: \" %(a, b, c, d))\n",
        "                model = SVD(n_factors = a, n_epochs = b, lr_all=c, reg_all=d)\n",
        "                model.fit(train)\n",
        "                predict = model.test(test)\n",
        "                rmse = accuracy.rmse(predict)\n",
        "                if rmse < min_error:\n",
        "                    min_error = rmse\n",
        "                    params = [a, b, c, d]\n",
        "                print(\"Current min_error is %s\" % min_error)\n",
        "print(min_error, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKE0jep5hzOw",
        "outputId": "b5d75e00-33fb-4bad-c6a9-fcc2b53bed21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 50 factors, 20 epochs, 0.005 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1446\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 20 epochs, 0.005 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1734\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 20 epochs, 0.01 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1793\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 20 epochs, 0.01 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1527\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 30 epochs, 0.005 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1585\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 30 epochs, 0.005 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1541\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 30 epochs, 0.01 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1953\n",
            "Current min_error is 1.144646411232415\n",
            "Training with 50 factors, 30 epochs, 0.01 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1388\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 20 epochs, 0.005 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1473\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 20 epochs, 0.005 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1691\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 20 epochs, 0.01 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1818\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 20 epochs, 0.01 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1485\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 30 epochs, 0.005 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1641\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 30 epochs, 0.005 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1496\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 30 epochs, 0.01 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1971\n",
            "Current min_error is 1.1387852998288894\n",
            "Training with 100 factors, 30 epochs, 0.01 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1343\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 20 epochs, 0.005 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1476\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 20 epochs, 0.005 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1659\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 20 epochs, 0.01 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1742\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 20 epochs, 0.01 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1464\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 30 epochs, 0.005 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1616\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 30 epochs, 0.005 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1475\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 30 epochs, 0.01 learning rate, 0.02 regularizer: \n",
            "RMSE: 1.1838\n",
            "Current min_error is 1.1343476646257598\n",
            "Training with 150 factors, 30 epochs, 0.01 learning rate, 0.1 regularizer: \n",
            "RMSE: 1.1326\n",
            "Current min_error is 1.132559527284982\n",
            "1.132559527284982 [150, 30, 0.01, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model and fit the trainset with\n",
        "tuned_model = SVD(n_factors = params[0], n_epochs = params[1], lr_all = params[2], reg_all = params[3])\n",
        "#tuned_model = SVD(n_factors = 150, n_epochs = 30, lr_all = 0.01, reg_all = 0.1)\n",
        "tuned_model.fit(train)\n",
        "\n",
        "#predicting on testset and evaluate\n",
        "predict = tuned_model.test(test)"
      ],
      "metadata": {
        "id": "0uxVyHVhh1SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the RMSE\n",
        "accuracy.rmse(predict)\n",
        "accuracy.mae(predict)"
      ],
      "metadata": {
        "id": "gxMEeUQyh23i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_predict_set(user_id, rating):\n",
        "    anime_id_list = rating['anime_id'].unique()\n",
        "    anime_watched = rating.loc[rating['user_id'] == user_id, 'anime_id']\n",
        "\n",
        "    rec_set = np.setdiff1d(anime_id_list, anime_watched)\n",
        "\n",
        "    return rec_set"
      ],
      "metadata": {
        "id": "qSEp7I0lh4cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_recs(model, test, user_id, anime, n_recs):\n",
        "\n",
        "    predict_set = [[user_id, anime_id, None] for anime_id in test]\n",
        "    predict = model.test(predict_set)\n",
        "    ratings = np.array([predictions.est for predictions in predict])\n",
        "\n",
        "    print(\"The top %s anime recommended for user_id %s:\" % (n_recs, user_id))\n",
        "    recs = (-ratings).argsort()[:n_recs]\n",
        "    for i in recs:\n",
        "        rec_name = test[i]\n",
        "        print(anime[anime['anime_id'] == rec_name]['name'].values[0])"
      ],
      "metadata": {
        "id": "scLT8VxMh6PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_user = random.choice(rating['user_id'])\n",
        "random_user\n",
        "\n",
        "test = create_predict_set(random_user, rating)\n",
        "num_recs = 5\n",
        "\n",
        "top_recs(tuned_model, test, random_user, anime, num_recs)"
      ],
      "metadata": {
        "id": "qEtArspGh8Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e549ae01-bf9d-496d-caf9-982339f54df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 5 anime recommended for user_id 53423:\n",
            "Clannad: After Story\n",
            "Fullmetal Alchemist: Brotherhood\n",
            "Steins;Gate\n",
            "Ginga Eiyuu Densetsu\n",
            "Hunter x Hunter (2011)\n"
          ]
        }
      ]
    }
  ]
}